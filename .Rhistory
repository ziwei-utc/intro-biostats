knitr::kable(c("Qualtitative","Quantitative"))
relfre <- data.frame(cbind(c(1,2,3,4,5,10,200,500,750,1000),c(1,2,2,2,2,4,111,257,378,487),c(1,1,.67,.5,.4,.4,.555,.514,.504,.487)), row.names = c("Number of tosses","Cumulative number of heads","Relative frequency of heads"))
cbind(c(1,2,3,4,5,10,200,500,750,1000),c(1,2,2,2,2,4,111,257,378,487),c(1,1,.67,.5,.4,.4,.555,.514,.504,.487))
relfre <- data.frame(cbind(c(1,2,3,4,5,10,200,500,750,1000),c(1,2,2,2,2,4,111,257,378,487),c(1,1,.67,.5,.4,.4,.555,.514,.504,.487)))
relfre
names(relfre)
names(relfre) <-  c("Number of tosses","Cumulative number of heads","Relative frequency of heads")
relfre
relfre <- data.frame(cbind(c(1,2,3,4,5,10,200,500,750,1000),c(1,2,2,2,2,4,111,257,378,487),c(1,1,.67,.5,.4,.4,.555,.514,.504,.487)))
names(relfre) <-  c("Number of tosses","Cumulative number of heads","Relative frequency of heads")
knitr::kable(relfre, "pipe")
knitr::include_graphics("prob-add.png")
knitr::include_graphics("albinism-ex.png")
knitr::include_graphics("sample-space.png")
knitr::include_graphics("medical-test.png")
library(xaringanBuilder)
build_pdf("C:\\Users\\djd955\\OneDrive - University of Tennessee\\Teaching\\2022 Fall\\MATH 5150\\notes\\my own\\lec-2.Rmd")
library(renderthis)
build_pdf("C:\\Users\\djd955\\OneDrive - University of Tennessee\\Teaching\\2022 Fall\\MATH 5150\\notes\\my own\\lec-2.Rmd")
to_pdf("C:\\Users\\djd955\\OneDrive - University of Tennessee\\Teaching\\2022 Fall\\MATH 5150\\notes\\my own\\lec-2.Rmd")
library(renderthis)
to_pdf("C:\\Users\\djd955\\OneDrive - University of Tennessee\\Teaching\\2022 Fall\\MATH 5150\\notes\\my own\\lec-1.Rmd")
knitr::opts_chunk$set(echo = TRUE)
# install.packages("wordcloud")
library(wordcloud)
# install.packages("RColorBrewer")
library(RColorBrewer)
# install.packages("wordcloud2")
library(wordcloud2)
# install.packages("tm")  # for text mining
library(tm)
library(NLP)
library(readr)
raw_data <- read_csv("C:/Users/djd955/Downloads/Reading-note-PIPP-Phase1-abstracts - Sheet1.csv")
one_wc <- function(id=1){
Raw$Abstract[id]
text <- Raw$Abstract[id]
# Load the data as a corpus
docs <- Corpus(VectorSource(text))
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
# Convert the text to lower case
docs <- tm_map(docs, content_transformer(tolower))
# Remove numbers
docs <- tm_map(docs, removeNumbers)
# Remove english common stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
# Remove your own stop word
# specify your stopwords as a character vector
docs <- tm_map(docs, removeWords, c("blabla1", "blabla2"))
# Remove punctuations
docs <- tm_map(docs, removePunctuation)
# Eliminate extra white spaces
docs <- tm_map(docs, stripWhitespace)
# Text stemming
# docs <- tm_map(docs, stemDocument)
docs$content
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=200, random.order=FALSE, rot.per=0.35,
scale=c(3.5,0.25),
colors=brewer.pal(8, "Dark2"),
size = 0.7, shape = 'pentagon')
}
one_wc <- function(id=1){
Raw$Abstract[id]
text <- Raw$Abstract[id]
# Load the data as a corpus
docs <- Corpus(VectorSource(text))
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
# Convert the text to lower case
docs <- tm_map(docs, content_transformer(tolower))
# Remove numbers
docs <- tm_map(docs, removeNumbers)
# Remove english common stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
# Remove your own stop word
# specify your stopwords as a character vector
docs <- tm_map(docs, removeWords, c("blabla1", "blabla2"))
# Remove punctuations
docs <- tm_map(docs, removePunctuation)
# Eliminate extra white spaces
docs <- tm_map(docs, stripWhitespace)
# Text stemming
# docs <- tm_map(docs, stemDocument)
docs$content
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=200, random.order=FALSE, rot.per=0.35,
scale=c(3.5,0.25),
colors=brewer.pal(8, "Dark2"),
size = 0.7, shape = 'pentagon')
}
one_wc(1)
one_wc <- function(id=1){
Raw$Abstract[id]
text <- Raw_data$Abstract[id]
# Load the data as a corpus
docs <- Corpus(VectorSource(text))
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
# Convert the text to lower case
docs <- tm_map(docs, content_transformer(tolower))
# Remove numbers
docs <- tm_map(docs, removeNumbers)
# Remove english common stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
# Remove your own stop word
# specify your stopwords as a character vector
docs <- tm_map(docs, removeWords, c("blabla1", "blabla2"))
# Remove punctuations
docs <- tm_map(docs, removePunctuation)
# Eliminate extra white spaces
docs <- tm_map(docs, stripWhitespace)
# Text stemming
# docs <- tm_map(docs, stemDocument)
docs$content
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=200, random.order=FALSE, rot.per=0.35,
scale=c(3.5,0.25),
colors=brewer.pal(8, "Dark2"),
size = 0.7, shape = 'pentagon')
}
one_wc(1)
raw <- read_csv("C:/Users/djd955/Downloads/Reading-note-PIPP-Phase1-abstracts - Sheet1.csv")
one_wc <- function(id=1){
Raw$Abstract[id]
text <- Raw$Abstract[id]
# Load the data as a corpus
docs <- Corpus(VectorSource(text))
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
# Convert the text to lower case
docs <- tm_map(docs, content_transformer(tolower))
# Remove numbers
docs <- tm_map(docs, removeNumbers)
# Remove english common stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
# Remove your own stop word
# specify your stopwords as a character vector
docs <- tm_map(docs, removeWords, c("blabla1", "blabla2"))
# Remove punctuations
docs <- tm_map(docs, removePunctuation)
# Eliminate extra white spaces
docs <- tm_map(docs, stripWhitespace)
# Text stemming
# docs <- tm_map(docs, stemDocument)
docs$content
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=200, random.order=FALSE, rot.per=0.35,
scale=c(3.5,0.25),
colors=brewer.pal(8, "Dark2"),
size = 0.7, shape = 'pentagon')
}
one_wc(1)
Raw <- read_csv("C:/Users/djd955/Downloads/Reading-note-PIPP-Phase1-abstracts - Sheet1.csv")
one_wc <- function(id=1){
Raw$Abstract[id]
text <- Raw$Abstract[id]
# Load the data as a corpus
docs <- Corpus(VectorSource(text))
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
# Convert the text to lower case
docs <- tm_map(docs, content_transformer(tolower))
# Remove numbers
docs <- tm_map(docs, removeNumbers)
# Remove english common stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
# Remove your own stop word
# specify your stopwords as a character vector
docs <- tm_map(docs, removeWords, c("blabla1", "blabla2"))
# Remove punctuations
docs <- tm_map(docs, removePunctuation)
# Eliminate extra white spaces
docs <- tm_map(docs, stripWhitespace)
# Text stemming
# docs <- tm_map(docs, stemDocument)
docs$content
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=200, random.order=FALSE, rot.per=0.35,
scale=c(3.5,0.25),
colors=brewer.pal(8, "Dark2"),
size = 0.7, shape = 'pentagon')
}
one_wc(1)
one_wc <- function(id=1){
Raw$Abstract[id]
text <- Raw$Abstract[id]
# Load the data as a corpus
docs <- Corpus(VectorSource(text))
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
# Convert the text to lower case
docs <- tm_map(docs, content_transformer(tolower))
# Remove numbers
docs <- tm_map(docs, removeNumbers)
# Remove english common stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
# Remove your own stop word
# specify your stopwords as a character vector
docs <- tm_map(docs, removeWords, c("blabla1", "blabla2"))
# Remove punctuations
docs <- tm_map(docs, removePunctuation)
# Eliminate extra white spaces
docs <- tm_map(docs, stripWhitespace)
# Text stemming
# docs <- tm_map(docs, stemDocument)
docs$content
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=50, random.order=FALSE, rot.per=0.35,
scale=c(3.5,0.25),
colors=brewer.pal(8, "Dark2"),
size = 0.7, shape = 'pentagon')
}
one_wc(1)
one_wc <- function(id=1){
Raw$Abstract[id]
text <- Raw$Abstract[id]
# Load the data as a corpus
docs <- Corpus(VectorSource(text))
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
# Convert the text to lower case
docs <- tm_map(docs, content_transformer(tolower))
# Remove numbers
docs <- tm_map(docs, removeNumbers)
# Remove english common stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
# Remove your own stop word
# specify your stopwords as a character vector
docs <- tm_map(docs, removeWords, c("blabla1", "blabla2"))
# Remove punctuations
docs <- tm_map(docs, removePunctuation)
# Eliminate extra white spaces
docs <- tm_map(docs, stripWhitespace)
# Text stemming
# docs <- tm_map(docs, stemDocument)
docs$content
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
set.seed(1234)
wordcloud = wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=50, random.order=FALSE, rot.per=0.35,
scale=c(3.5,0.25),
colors=brewer.pal(8, "Dark2"),
size = 0.7, shape = 'pentagon')
return(d)
}
one_wc(1)
head(one_wc(1))
head(one_wc(2))
head(one_wc(3))
for (id in 1:20) {
Raw$Abstract[id]
text <- Raw$Abstract[id]
# Load the data as a corpus
docs <- Corpus(VectorSource(text))
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
# Convert the text to lower case
docs <- tm_map(docs, content_transformer(tolower))
# Remove numbers
docs <- tm_map(docs, removeNumbers)
# Remove english common stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
# Remove your own stop word
# specify your stopwords as a character vector
docs <- tm_map(docs, removeWords, c("blabla1", "blabla2"))
# Remove punctuations
docs <- tm_map(docs, removePunctuation)
# Eliminate extra white spaces
docs <- tm_map(docs, stripWhitespace)
# Text stemming
# docs <- tm_map(docs, stemDocument)
docs$content
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=200, random.order=FALSE, rot.per=0.35,
scale=c(3.5,0.2),
colors=brewer.pal(8, "Dark2"),
size = 0.7, shape = 'pentagon')
}
one_wc(4)
one_wc(5)
one_wc(6)
one_wc(7)
one_wc(8)
one_wc(9)
one_wc(10)
Raw$Abstract
attr(Raw$Abstract)
?attr
attr(Raw)
attr(Raw,"f")
attr(Raw,"foo")
m1 <- c(Raw$Abstract[1],Raw$Abstract[2])
m1
m1 <- list(Raw$Abstract[1],Raw$Abstract[2])
m1
m1[[1]]
m1[[2]]
m1[1]
m1[[1]]
m1[[[1]]]
m1[[1]
]
n <- length(Raw$Abstract)
n
# print top ten word in each abstract
one_wc(1)[1:10]
# print top ten word in each abstract
one_wc(1)
# print top ten word in each abstract
one_wc(1)[1:0,]
# print top ten word in each abstract
one_wc(1)[1:10,]
library(dplyr)
library(proxy)
install.packages("proxy")
library(dplyr)
library(proxy)
library(stringr)
library(data.table)
## prepare the text
doc1 <- list(Raw$Abstract[1],Raw$Abstract[2])
## prepare the text
doc1 <- list(Raw$Abstract[1],Raw$Abstract[2])
for (i in 3:20) {
doc1[i] <- Raw$Abstract[i]
}
doc1
## prepare the text
doc <- list(Raw$Abstract[1],Raw$Abstract[2])
for (i in 3:20) {
doc[i] <- Raw$Abstract[i]
}
## preprocess text
doc1 <- lapply(doc, function(x) {
text <- gsub("[[:punct:]]", "", x) %>% tolower()
text <- gsub("\\s+", " ", text) %>% str_trim()
word <- strsplit(text, " ") %>% unlist()
return(word)
})
doc1[[1]]
## Shingling
Shingling <- function(document, k) {
shingles <- character( length = length(document) - k + 1 )
for( i in 1:( length(document) - k + 1 ) ) {
shingles[i] <- paste( document[ i:(i + k - 1) ], collapse = " " )
}
return( unique(shingles) )
}
# "shingle" our example document, with k = 3
doc1 <- lapply(doc1, function(x) {
Shingling(x, k = 3)
})
list( Original = doc[[1]], Shingled = doc1[[1]] )
## # unique shingles sets across all documents
doc_dict <- unlist(doc1) %>% unique()
# "characteristic" matrix
M <- lapply(doc1, function(set, dict) {
as.integer(dict %in% set)
}, dict = doc_dict) %>% data.frame()
# set the names for both rows and columns
setnames( M, paste( "doc", 1:length(doc1), sep = "_" ) )
rownames(M) <- doc_dict
M
# how similar is two given document, jaccard similarity
JaccardSimilarity <- function(x, y) {
non_zero <- which(x | y)
set_intersect <- sum( x[non_zero] & y[non_zero] )
set_union <- length(non_zero)
return(set_intersect / set_union)
}
# create a new entry in the registry
pr_DB$set_entry( FUN = JaccardSimilarity, names = c("JaccardSimilarity") )
# jaccard similarity distance matrix
d1 <- dist( t(M), method = "JaccardSimilarity" )
# delete the new entry
pr_DB$delete_entry("JaccardSimilarity")
d1
heatmap(d1)
d1
heatmap(as.matrix(d1))
heatmap(as.matrix(d1),, scale="column")
heatmap(as.matrix(d1),, scale="column",col= colorRampPalette(brewer.pal(8, "Blues"))(25))
heatmap(as.matrix(d1), scale="column",col= colorRampPalette(brewer.pal(8, "Blues"))(25))
overall <- paste(doc)
overall
# overall word cloud
overall <- paste(unlist(doc))
overall
# overall word cloud
remove(overall)
unlist(doc)
doctest <- unlist(doc)
doctest[1]
doctest[2]
doctest
doctest <- paste(doc[1],doc[2])
doctest
# overall word cloud
remove(overall)
overall <- paste(doc[1],doc[2])
for (i in 3:20){
overall <- paste(overall, doc[i])
}
## word cloud for overall text
text <- overall
# Load the data as a corpus
docs <- Corpus(VectorSource(text))
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
# Convert the text to lower case
docs <- tm_map(docs, content_transformer(tolower))
# Remove numbers
docs <- tm_map(docs, removeNumbers)
# Remove english common stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
# Remove your own stop word
# specify your stopwords as a character vector
docs <- tm_map(docs, removeWords, c("blabla1", "blabla2"))
# Remove punctuations
docs <- tm_map(docs, removePunctuation)
# Eliminate extra white spaces
docs <- tm_map(docs, stripWhitespace)
# Text stemming
# docs <- tm_map(docs, stemDocument)
docs$content
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
set.seed(1234)
wordcloud = wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=50, random.order=FALSE, rot.per=0.35,
scale=c(3.5,0.25),
colors=brewer.pal(8, "Dark2"),
size = 0.7, shape = 'pentagon')
head(d1,20)
head(d,20)
knitr::opts_chunk$set(echo = TRUE, fig.width = 3,fig.height = 3, fig.align="center")
knitr::include_graphics("ppo-sam.png")
knitr::include_graphics("pop-sam.png")
knitr::include_graphics("pop-not.png")
knitr::include_graphics("pop-sam.png", "pop-not.png")
knitr::include_graphics(c("pop-sam.png", "pop-not.png"))
knitr::include_graphics(c("pop-sam.png", "pop-not.png"))
knitr::include_graphics("pop-sam.png")
